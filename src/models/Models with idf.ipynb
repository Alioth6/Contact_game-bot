{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26861, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>defs</th>\n",
       "      <th>word</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>колпак на лампе</td>\n",
       "      <td>абажур</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>настоятель католического монастыря</td>\n",
       "      <td>аббат</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>католический священник</td>\n",
       "      <td>аббат</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>католический монастырь</td>\n",
       "      <td>аббатство</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>слово из первых букв</td>\n",
       "      <td>аббревиатура</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 defs          word  len\n",
       "0                     колпак на лампе        абажур   15\n",
       "1  настоятель католического монастыря         аббат   34\n",
       "2              католический священник         аббат   22\n",
       "3              католический монастырь     аббатство   22\n",
       "4                слово из первых букв  аббревиатура   20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_name = 'test_set_002.csv'\n",
    "\n",
    "data = pd.read_csv(file_name, delimiter=',')\n",
    "\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "nlp = spacy.load('ru2')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), first=True)\n",
    "stops = stopwords.words(\"russian\")\n",
    "doc_list = []\n",
    "\n",
    "\n",
    "for text in data['defs']:\n",
    "    text = ''.join(x for x in text if x not in punctuation)\n",
    "    words = word_tokenize(text, language=\"russian\")\n",
    "    words = [word for word in words if word not in stops]\n",
    "    text = ' '.join(words)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join(word.lemma_ for word in doc)\n",
    "    doc_list.append(text)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(doc_list)\n",
    "\n",
    "vocab = vectorizer.vocabulary_\n",
    "arr_of_idf = vectorizer.idf_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tf_idf(word):\n",
    "    if word in vocab:\n",
    "        return arr_of_idf[vocab[word]]\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = []\n",
    "\n",
    "for word in data['word']:\n",
    "    dt.append(word)\n",
    "    \n",
    "for text in doc_list:\n",
    "    dt.append(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "деревце 0.867724597454071\n",
      "деревцо 0.8552745580673218\n",
      "дерево, — 0.8466764688491821\n",
      "дерево… 0.8452415466308594\n",
      "срубленное 0.8443728685379028\n",
      "росшее 0.8330211639404297\n",
      "развесистое 0.8261986970901489\n",
      "лиственное 0.8218693733215332\n",
      "сучковатое 0.8129539489746094\n",
      "раскидистое 0.8127985000610352\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_fpath = \"all.norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore', limit=500000)\n",
    "w2v_model.init_sims(replace=True)\n",
    "for word, score in w2v_model.most_similar(\"дерево\"):\n",
    "    print(word, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "idf_w2v_model = Word2Vec(size=100)\n",
    "idf_w2v_model.build_vocab(w2v_model.vocab)\n",
    "\n",
    "for word in w2v_model.vocab:\n",
    "    idf_w2v_model.wv[word] = w2v_model[word] * get_word_tf_idf(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of idf-vectors with cosine metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: sum of idf-vectors\n",
    "\n",
    "\n",
    "def get_words(sentence, prefix=\"\"):\n",
    "    \n",
    "    sentence = ''.join(x for x in sentence if x not in punctuation)\n",
    "    doc = nlp(sentence)\n",
    "    sentence = ' '.join(word.lemma_ for word in doc)\n",
    "    words = word_tokenize(sentence, language=\"russian\")\n",
    "    words = [word for word in words if word not in stops]\n",
    "    words = [word for word in words if word in idf_w2v_model.wv.vocab]\n",
    "    \n",
    "    if words != []:\n",
    "        sum_similar = idf_w2v_model.wv.most_similar(positive=words, topn=40)\n",
    "        res = [i[0] for i in sum_similar if prefix == i[0][:len(prefix)]]\n",
    "        return res\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 4.314805852350992 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>defs</th>\n",
       "      <th>word</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>легкое агрегатное состояние</td>\n",
       "      <td>газ</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>оскорбление самого значимого</td>\n",
       "      <td>святотатство</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>разновидность чего-либо</td>\n",
       "      <td>род</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>возвышенность наоборот</td>\n",
       "      <td>низина</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>скрученная еда</td>\n",
       "      <td>рулет</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           defs          word  len\n",
       "0   легкое агрегатное состояние           газ   58\n",
       "1  оскорбление самого значимого  святотатство   53\n",
       "2       разновидность чего-либо           род   42\n",
       "3        возвышенность наоборот        низина   20\n",
       "4                скрученная еда         рулет   63"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'test_set_human_003.csv'\n",
    "\n",
    "human_data = pd.read_csv(file, comment='#')\n",
    "\n",
    "print(human_data.shape)\n",
    "human_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 8.004158004158004 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 8.871598227914076 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 17.463617463617464 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean of idf-vectors with cosine metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: mean of idf-vectors\n",
    "\n",
    "\n",
    "def get_words_1(sentence, prefix=\"\"):\n",
    "    \n",
    "    sentence = ''.join(x for x in sentence if x not in punctuation)\n",
    "    doc = nlp(sentence)\n",
    "    sentence = ' '.join(word.lemma_ for word in doc)\n",
    "    words = word_tokenize(sentence, language=\"russian\")\n",
    "    words = [word for word in words if word not in stops]\n",
    "    words = [word for word in words if word in idf_w2v_model.wv.vocab]\n",
    "    \n",
    "    if words != []:\n",
    "        sum_similar = idf_w2v_model.wv.most_similar(positive=words, topn=40)\n",
    "        res = [i[0] for i in sum_similar if prefix == i[0][:len(prefix)]]\n",
    "        res.sort(key=lambda x: cosine_dist_between_mean_word(words, x), reverse=True)\n",
    "        return res\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 5.092885596217565 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words_1(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 7.900207900207901 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words_1(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 9.992181973865454 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words_1(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 16.943866943866944 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words_1(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of idf-vectors with Euclidean metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_dist(words, curr_word):\n",
    "    arr = []\n",
    "    for word in words:\n",
    "        arr.append(idf_w2v_model.wv[word])\n",
    "    \n",
    "    np_arr = np.array(arr)\n",
    "    sum_vector = np_arr.sum(axis=0)\n",
    "    return np.linalg.norm(sum_vector - idf_w2v_model.wv[curr_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: sum of idf-vectors\n",
    "\n",
    "\n",
    "def get_words2(sentence, prefix=\"\"):\n",
    "    \n",
    "    sentence = ''.join(x for x in sentence if x not in punctuation)\n",
    "    doc = nlp(sentence)\n",
    "    sentence = ' '.join(word.lemma_ for word in doc)\n",
    "    words = word_tokenize(sentence, language=\"russian\")\n",
    "    words = [word for word in words if word not in stops]\n",
    "    words = [word for word in words if word in idf_w2v_model.wv.vocab]\n",
    "    \n",
    "    if words != []:\n",
    "        sum_similar = idf_w2v_model.wv.most_similar(positive=words, topn=75)\n",
    "        res = [i[0] for i in sum_similar if prefix == i[0][:len(prefix)]]\n",
    "        res.sort(key=lambda x: euclidean_dist(words, x))\n",
    "        return res\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 4.608912549793381 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words2(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 10.81081081081081 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words2(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 8.383902311902014 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words2(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 17.775467775467778 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words2(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean of idf-vectors with Euclidean metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist_between_mean(words, curr_word):\n",
    "    arr = []\n",
    "    for word in words:\n",
    "        arr.append(idf_w2v_model.wv[word])\n",
    "    \n",
    "    np_arr = np.array(arr)\n",
    "    sum_vector = np_arr.sum(axis=0)\n",
    "    mean_vector = np.divide(sum_vector, len(words))\n",
    "    \n",
    "    return np.linalg.norm(mean_vector - idf_w2v_model.wv[curr_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: mean of idf-vectors\n",
    "\n",
    "\n",
    "def get_words3(sentence, prefix=\"\"):\n",
    "    \n",
    "    sentence = ''.join(x for x in sentence if x not in punctuation)\n",
    "    doc = nlp(sentence)\n",
    "    sentence = ' '.join(word.lemma_ for word in doc)\n",
    "    words = word_tokenize(sentence, language=\"russian\")\n",
    "    words = [word for word in words if word not in stops]\n",
    "    words = [word for word in words if word in idf_w2v_model.wv.vocab]\n",
    "    \n",
    "    if words != []:\n",
    "        sum_similar = idf_w2v_model.wv.most_similar(positive=words, topn=75)\n",
    "        res = [i[0] for i in sum_similar if prefix == i[0][:len(prefix)]]\n",
    "        res.sort(key=lambda x: euclidean_dist_between_mean(words, x))\n",
    "        return res\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 3.041584453296601 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words3(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 6.652806652806653 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=3\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words3(text)[:3]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_dataset score is 6.567141953017385 %\n",
      "There were  1299  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on wiki_dataset (test_set_002.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    word = data['word'][i]\n",
    "    text = data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "        \n",
    "    if word in get_words3(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(data)) * 100\n",
    "print('wiki_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dataset score is 13.097713097713099 %\n",
      "There were  33  words not from vocabulary\n"
     ]
    }
   ],
   "source": [
    "# testing on human_dataset (test_set_human_003.csv) topn=10\n",
    "\n",
    "guessed = 0\n",
    "words_non_in_dict = 0\n",
    "\n",
    "for i in range(len(human_data)):\n",
    "    word = human_data['word'][i]\n",
    "    text = human_data['defs'][i]\n",
    "    \n",
    "    if word not in idf_w2v_model.wv.vocab:\n",
    "        words_non_in_dict += 1\n",
    "        continue\n",
    "    \n",
    "    if word in get_words3(text)[:10]:\n",
    "        guessed += 1\n",
    "\n",
    "success = (guessed / len(human_data)) * 100\n",
    "print('human_dataset score is', success, '%')\n",
    "print('There were ', words_non_in_dict, ' words not from vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Wiki, topn=3</th>\n",
       "      <th>Wiki, topn=10</th>\n",
       "      <th>Human, topn=3</th>\n",
       "      <th>Human, topn=10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sum, cosine metric</td>\n",
       "      <td>score is 4.315%, 1299 words not from vocab</td>\n",
       "      <td>score is 8.8716%</td>\n",
       "      <td>score is 8.004%, 33  words not from vocab</td>\n",
       "      <td>score is 17.464%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mean, cosine metric</td>\n",
       "      <td>score is 5.093%, 1299 words not from vocab</td>\n",
       "      <td>score is 9.992%</td>\n",
       "      <td>score is 7.900%, 33  words not from vocab</td>\n",
       "      <td>score is 16.944%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sum, euclidean metric</td>\n",
       "      <td>score is 4.609%, 1299 words not from vocab</td>\n",
       "      <td>score is 8.384%</td>\n",
       "      <td>score is 10.811%, 33  words not from vocab</td>\n",
       "      <td>score is 17.775%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mean, euclidean metric</td>\n",
       "      <td>score is 3.042%, 1299 words not from vocab</td>\n",
       "      <td>score is 6.567%</td>\n",
       "      <td>score is 6.653%, 33  words not from vocab</td>\n",
       "      <td>score is 13.098%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model                                Wiki, topn=3  \\\n",
       "0      Sum, cosine metric  score is 4.315%, 1299 words not from vocab   \n",
       "1     Mean, cosine metric  score is 5.093%, 1299 words not from vocab   \n",
       "2   Sum, euclidean metric  score is 4.609%, 1299 words not from vocab   \n",
       "3  Mean, euclidean metric  score is 3.042%, 1299 words not from vocab   \n",
       "\n",
       "      Wiki, topn=10                               Human, topn=3  \\\n",
       "0  score is 8.8716%   score is 8.004%, 33  words not from vocab   \n",
       "1   score is 9.992%   score is 7.900%, 33  words not from vocab   \n",
       "2   score is 8.384%  score is 10.811%, 33  words not from vocab   \n",
       "3   score is 6.567%   score is 6.653%, 33  words not from vocab   \n",
       "\n",
       "     Human, topn=10  \n",
       "0  score is 17.464%  \n",
       "1  score is 16.944%  \n",
       "2  score is 17.775%  \n",
       "3  score is 13.098%  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = ['Model', 'Wiki, topn=3', 'Wiki, topn=10', 'Human, topn=3', 'Human, topn=10']\n",
    "\n",
    "array = [['Sum, cosine metric', 'score is 4.315%, 1299 words not from vocab', 'score is 8.8716%', 'score is 8.004%, 33  words not from vocab', 'score is 17.464%'],\n",
    "       ['Mean, cosine metric', 'score is 5.093%, 1299 words not from vocab', 'score is 9.992%', 'score is 7.900%, 33  words not from vocab', 'score is 16.944%'],\n",
    "       ['Sum, euclidean metric', 'score is 4.609%, 1299 words not from vocab', 'score is 8.384%', 'score is 10.811%, 33  words not from vocab', 'score is 17.775%'],\n",
    "       ['Mean, euclidean metric', 'score is 3.042%, 1299 words not from vocab', 'score is 6.567%', 'score is 6.653%, 33  words not from vocab', 'score is 13.098%']\n",
    "      ]\n",
    "\n",
    "table = pd.DataFrame(data = array, columns = entries)\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
